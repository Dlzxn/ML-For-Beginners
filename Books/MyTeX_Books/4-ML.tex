%! Author = alex
%! Date = 04.07.2025

    \centering
    \vspace{1cm}
\section*{\Huge \textbf{Глава 4. Введение в Машинное обучение}}
\vspace{0.5cm}

    \vspace{1em}
\noindent\rule{\linewidth}{0.4pt}

\vspace{1em}
\textbf{ Основные задачи ML:}
\begin{itemize}
    \item \textbf{Классификация} — определение объектов к определённым классам по общим признакам
    \item \textbf{Регрессия} — прогнозирование величин, функций или событий
    \item \textbf{Ранжирование} — упорядочивание входного набора данных
\end{itemize}

\vspace{1em}
\noindent\rule{\linewidth}{0.4pt}

%------------------------------------------------------------------------------------1.1
\section*{\S 1.1 Обучающая выборка}

Представление объектов в виде различных векторов данных:

\[
    X = [x_1, x_2, \ldots, x_n]^T =  \begin{bmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{bmatrix}
\]
X - вектор входных данных

\textbf{Допустим, у нас дана матрица:}

\[A =
\begin{bmatrix}
x_{11} & x_{12} & \ldots & x_{1n} \\
x_{21} & x_{22} & \ldots & x_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
x_{m1} & x_{m2} & \ldots & x_{mn}
\end{bmatrix}
\]
\raggedright
Здесь A - матрица входных данных, \( n \) — количество признаков объекта, а \( m \) — количество самих объектов.

Таким же видом представлены и выходные данные:

\[
    Y = [y_1, y_2, ... , y_n ]^T = \begin{bmatrix}
                                       y_1 \\
                                       y_2 \\
                                       ...\\
                                       y_m
                                       \end{bmatrix}
\]
Y - вектор выходных данных

\newpage
Теперь мы рассмотрим важный вопрос: как же такие обьекты как изображения, звук и т. д. могут представляться в виде векторов? \\
\vspace{1cm}
    Допустим, на вход задаче подается Изображение:
    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.4\textwidth]{images/snake.png}
        \label{fig:example}
    \end{figure}
\\
    Теперь важное замечание-\textbf{размерность вектора n будет зависеть от количества пикселей в изображении} \\
    \vspace{1cm}
    Например, если изображение 1024 на 256, то размерность вектора будет 1024*256 = 262144
    \vspace{0.9cm}
    \[X = [x_1, x_2, ... , x_{262144} ]
    \]

    \centering
    \vspace{1cm}
    Теперь \textbf{объединим} эти понятия:
    \[
        X^{'} = \{(x_i, y_i ) | 0 < i < m\} - \textbf{ размеченные данные(обучающая выборка)}
    \]
    Это и является одним из важнейших понятий в области машинного обучения, с которым вы будете встречаться всюду.
\noindent\rule{\linewidth}{0.4pt}

%------------------------------------------------------------------------------------1.2
    \newpage
    \centering
    \section*{\S 1.2 Постановка задачи для модели}
    \vspace{1em}
\raggedright
    А теперь разберемся с тем, как же модель будет "обучаться": \\
    \vspace{0.8cm}
    Допустим, у нас есть размеченные данные (x_i, y_i), которые подаются в некоторую модель

\usetikzlibrary{shapes.geometric, arrows.meta}

\tikzset{
    block/.style = {rectangle, draw, fill=blue!20, text centered, minimum height=3em, minimum width=6em},
    arrow/.style = {thick, -{Latex[length=3mm]}},
    data/.style = {circle, draw, fill=green!15, minimum size=2em, inner sep=1pt, text centered}
}


\begin{center}
\begin{tikzpicture}[node distance=2cm and 2.5cm]

% Входные данные
\node[data] (x) {\(x_i\)};
\node[data, below=of x] (y) {\(y_i\)};

% Блок модели
\node[block, right=3cm of $(x)!0.5!(y)$] (model) {Модель};

% Выход
\node[data, right=4cm of model] (output) {Предсказание};

% Стрелки
\draw[arrow] (x) -- ([yshift=+0.8em]model.west);
\draw[arrow] (y) -- ([yshift=-0.8em]model.west);
\draw[arrow] (model) -- (output);

\end{tikzpicture}
\end{center}

В результате из исходных данных мы получили некое предсказание, которое на первых этапах обучения может не иметь ничего общего с правильным ответом. \\
\vspace{1cm}
    Теперь представим нашу модель как линейную функцию:
    \[y(x) = \phi (x, \Delta)  \ \ \ \ \ \  (1)\]
    Здесь \Delta - \textbf{постоянно меняющийся параметр} \\
    Его мы будем подстраивать для наиболее точного ответа нашей модели \\
    \vspace{1cm}
    Для лучшего понимания перейдем к задаче линейной регрессии.
    Задана функция:
    \[
        y(x, k, b) = kx + b + \psi
    \]
    Здесь k и b - параметры от которых зависит угол поворота прямой а так же ее сдвиг
    Т.е. получается, что эта прямая может проходить как угодно, но за счет размеченных данных мы задаем модели желаемый результат:

\begin{figure}[htbp]
        \centering
        \includegraphics[width=0.4\textwidth]{images/lin_regress.png}
        \label{fig:example}
    \end{figure}
    И получается, что во время обучения модель дает прогнозы все точнее и точнее к желаемому результату. \\
    \vspace{1cm}
    Но как же наш алгоритм понимает, что ответ надо корректировать?\\
    Сейчас мы подошли к еще одному очень важному определению в области ML: \\
    \vspace{0.8cm}
    \textbf{Функция потерь - функция, которая характеризует потери при неправильном предсказании модели}
    Примеры таких функций:
    \[
    L(x, a) = |a(x) - y(x)| \quad \text{— абсолютная ошибка}
\]
\[
    L(x, a) = (a(x) - y(x))^2 \quad \text{— квадратичная ошибка}
\]

\vspace{0.5cm}

Так же введем связное понятие:
\begin{tcolorbox}[colback=gray!10, colframe=black, title=Средний эмпирический риск]
\[
    Q(a) = \frac{1}{l} \sum_{i=1}^l L(a(x_i), y_i)
\]
Среднее значение функции потерь на обучающей выборке.
\end{tcolorbox}


    Это средне арифметическое по всем потерям в текущем цикле обучения модели. \\
    \vspace{0.5cm}
    Вспомним формулу (1):
    Наша задача - минимизировать средний эмпирический риск за счет изменения параметра \Delta \\
\vspace{1.5cm}



%------------------------------------------------------------------------------------1.3

\centering
\section*{\S 1.3 Линейная модель}
\vspace{0.8cm}
\raggedright

Рассмотрим функцию \( y = kx + b + \psi \).

В процессе обучения модели на данной функции перед нами
будет стоять задача подобрать такие \( k^{\prime} \) и \( b^{\prime} \),
чтобы сама функция \( y \) наиболее точно отображала желаемый результат.

\vspace{0.5cm}

Но что если мы попробуем выразить функцию через характеристики объекта?

Допустим, у нас есть \( \phi_1 \), \( \phi_2 \):

\[
    y = f_1(x)\phi_1 + f_2(x)\phi_2 + \psi
\]

Здесь \( f_1(x) \) — первая характеристика объекта, а \( f_2(x) \) — вторая.

Очевидно, что если \( f_1(x) = x \), \( f_2(x) = 1 \), а \( \phi_1 = k^{\prime} \), \( \phi_2 = b^{\prime} \),
то формула сводится к изначальной:

\[
    y(x) = k^{\prime}x + b^{\prime}
\]

Итак, линейная модель:
\[
    a(x) = \sum_{i=0}^{n} f_i(x)\phi_i
\]

Рассмотрим конкретный пример


\centering
\section*{\S 1.3 BETA Переобучение}
\vspace{0.8cm}
\raggedright

Нам дана функция:
\[
    f(x) = x^2 + x + 1
\]
\begin{figure}[htbp]
        \includegraphics[width=0.4\textwidth]{images/linear_data.png}
        \label{fig:example}
    \end{figure}

Сначала может показаться, что мы можем описать данную функцию с
помощью одной характеристики: \(a(x) = f(x) \phi\)\\
\begin{figure}[htbp]
        \includegraphics[width=0.4\textwidth]{images/error_data.png}
        \label{fig:example}
    \end{figure}
Но в таком случае получится прямая линия, лишь по очертаниям похожая на нашу кривую. \\
\vspace{0.7cm}
В таком случае нам поможет полином: \[
\begin{cases}
    f_0(x) = \text{const} \\
    f_1(x) = x \\
    f_2(x) = x^2 \\
    \vdots \\
    f_n(x) = x^n
\end{cases}
\quad \textbf{-- все характеристики}
\]


Таким образом наша модель(при \(f_{0}\) = 1): \(a(x) = \sum_{i = 0}^n\ f_i(x) \phi_i =
\phi_0 + x\phi_1 + .... + x^n \phi_n\) \\
\begin{tcolorbox}[colback=gray!10, colframe=black, title=Важно]
    Система характеристик является \textbd{Линейно Независимой} \\
\end{tcolorbox}

\textbf{Почему характеристики не могут быть линейно зависимыми?} \\
\vspace{0.5cm}

Допустим задана система:
\[
    \begin{cases}
        f_0(x) = 1 \\
        f_1(x) = x \\
        f_2(x) = x+5
    \end{cases}
\]
Получается, что \(f_2(x) = 5f_1(x) + f_1(x)\)
Следовательно, если одну из характеристик можно выразить через другие, то зачем же  он вообще нужен? Получается, что
он просто является лишним в нашей системе и можно справиться без него.

#ПЕРЕПИСАТЬ ГЛАВУ

\newpage
\centering
\section*{\S 1.4 Степень переобучения модели} \\
\vspace{0.8cm}
\raggedright

\textbf{1.4.1 Оценка по отложенной выборке(hold-out)} \\
\vspace{0.5cm}
Для данного метода размеченные данные делят на две части: \\
\vspace{0.2cm}
\begin{itemize}
    \item Обучающие
    \item Отложенная(hold-out)
\end{itemize}
\textbf{Обычно данные делят в соотношении 70:30.}

Цель: сравнить качество модели на данных, используемых при обучении, с новыми данными того же характера.
Для этого строят два новых параметра: \textbf{\(Q(a, X)\) - для обучающих данных и \(Q^{'}(a, X^{'})\) - для отложенных данных.} \\
\vspace{0.3cm}

В результате, если средним эмпирический риск для обучающих данных меньше, чем для отложенных,
то модель следует подкорректировать для лучшего показателя. \\
\vspace{0.5cm}

\textbf{1.4.2 Скользящий контроль(leave-one-out)} \\
\vspace{0.5cm}

Допустим, нам даны n различных размеченных данных : \X = (x_1, x_2, ... , x_n)\) \\
Данный метод основан на том, что мы построим n таких моделей, что : \\
\vspace{0.3cm}
\begin{cases}
    a_1(x): X_1 = (x_2, x_3, ..., x_n) \\
    a_2(x): X_2 = (x_1, x_3, x_4, ..., x_n) \\
    .... .................................. \\
    a_n(x): X_n = {x_1, x_2, ...., x_{n-1}}
\end{cases} \\
\vspace{0.2cm}
Т.е. мы построили n различных моделей \(a_i(x)\), таких, что каждая из них обучалась на наборе
данных размерностью \(n-1\) (для \(a_i-ой\) модели убирали \(x_i\)-ый вектор данных )\\
\vspace{0.3cm}

Ну а конечная модель a(x) = \(F(a_1, a_2, ..., a_n)\) \\
\vspace{0.3cm}

На больших наборах данных этот способ требует огромной вычислительной мощи,
потому он почти не используется на практике. \\
\vspace{0.5cm}
\textbf{1.4.3 Кросс-валидация(cross-validation, k-fold)} \\
\vspace{0.5cm}
Очень похожий на скользящий контроль метод, но различие состоит в том, что здесь мы разбиваем входные данные
на некоторые группы и составляем из них модели: \\
\begin{figure}[htbp]
        \centering
        \includegraphics[width=0.3\textwidth]{images/sxema.png}
        \label{fig:example}
\end{figure} \\
Этот метод позволяет строить модели, которые будут обладать \(\textbf{лучшими обобщающими способностями,
при меньше количестве вычислений.}\)

\vspace{0.5cm}
\textbf{1.4.4 Обобщение моделей} \\
\vspace{0.5cm}
В прошлых частях мы встречались с обобщением модели: \(a(x) = F(a_1(x), a_2(x), ... , a_n(x))\), но не упоминалось,
как же это делается на самом деле.
В основном используют два метода для обобщения моделей, познакомимся с ними поближе: \\
\vspace{0.5cm}

1) \(\textbf{Выбор одной модели с лучшими показателями}\) \\
\vspace{0.3cm}
Допустим у нас есть модели: \\
\[
    \begin{cases}
    a_1(x) \\
    a_2(x) \\
    .... . \\
    a_n(x)
\end{cases} \\
\]
Среди них мы выбираем ту модель, у которой \(\textbf{средний эмпирический риск минимальный}\).
Но на самом деле, даже если этот показатель и наименьший, это не означает, что модель лучше остальных. \\
\vspace{0.3cm}
2) \(\textbf{Выбор наиболее часто встречающегося результата}\) \\
\vspace{0.3cm}
Допустим у нас есть модели, которые выдают определенный результат \(\in (0, 1)\): \\
\[
    \begin{cases}
    a_1(x) = 1 \\
    a_2(x) = 1 \\
    .... \\
    a_{n-1}(x) = 1 \\
    a_n(x) = 0
\end{cases} \\
\]
Чаще всего встречается ответ 1, а значит мы его примем за верный. \\
Но, данный способ \(\textbf{требует большой вычислительной мощи}\),
т к предсказания нам будет давать уже не одна а n моделей. \\

\newpage
\centering
\section*{\S 1.5 Уравнение гиперплоскости} \\
\vspace{0.8cm}
\raggedright
Рассмотрим
\begin{figure}[htbp]
        \centering
        \includegraphics[width=0.45\textwidth]{images/examples_graph_4.png}
        \label{fig:example_4}
\end{figure}
Здесь прямая в двумерном пространстве делит два класса предметов, т.е. по левую часть от прямой располагаются предметы,
относящиеся к одному классе, а по правую-к другому классу. \\
\vspace{0.5cm}

Обратимся к линейному уравнению:
\[
    w_1 x_1 + w_2 x_2 + w_0 = 0
\]
Вектор \(w = (w_1, w_2)^T\) является нормалью к гиперплоскости, т.е. ортогонален всем векторам, лежащим в этой плоскости.


\begin{figure}[htbp]
        \centering
        \includegraphics[width=0.45\textwidth]{images/examples_graph_41.png}
        \label{fig:example_41}
\end{figure} \\
\vspace{0.5cm}

\textbf{Докажем это:} \\

Пусть \(x\) — произвольный вектор, лежащий в гиперплоскости. Тогда:

\[
w \cdot x = \|w\| \|x\| \cos(\theta)
\]

Поскольку \(x\) лежит в гиперплоскости, а \(w\) — нормаль, угол между ними \(90^\circ\), следовательно:

\[
\cos(\theta) = 0 \Rightarrow w \cdot x = 0
\]


\blacksquare

\vspace{0,5cm}
Но как же  нам теперь отличать объекты одного класса от объектов другого класса с использованием полученных знаний?

Все очень просто, рассмотрим углы между ортогональной к плоскости прямой и прямой до объекта.
\begin{figure}[htbp]
        \centering
        \includegraphics[width=0.45\textwidth]{images/examples_graph_42.png}
        \label{fig:example_42}
\end{figure} \\

Заметим, что угол a является \(\textbf{острым углом}\), как и любой другой угол между нормальную к гиперплоскости
и прямой до объекта такого же класса.(cos(a) > 0)\\
\vspace{0.3cm}

а вот угол b является является \(\textbf{тупым}\), как и любой другой угол между нормальную к гиперплоскости
и прямой до объекта такого же класса.(cos(a) < 0) \\
\vspace{0.3cm}

А значит скалярное произведение (w, x) для одного класса будет положительным,
а для другого отрицательным(следует из знака cos) \\
Более того, мы можем описать такую функцию как:
\[
    a(x, w) = sign((w, x)) = \begin{cases}
                                -1, \ (w, x) < 0 \\
                                \ 1,  \ \ \ (w, x) > 0
                                \end{cases}
\]
При нуле объект не будет определен к ни к одному классу.

